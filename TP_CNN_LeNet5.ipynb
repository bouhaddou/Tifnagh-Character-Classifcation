{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8faf9-dbd4-4df0-883c-a2ac9d0e4f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: X=(20290, 32, 32, 1), y=(20290, 33)\n",
      "Classes: ['ya' 'yab' 'yach' 'yad' 'yadd' 'yae' 'yaf' 'yag' 'yagg' 'yagh' 'yah'\n",
      " 'yahh' 'yaj' 'yak' 'yakk' 'yal' 'yam' 'yan' 'yaq' 'yar' 'yarr' 'yas'\n",
      " 'yass' 'yat' 'yatt' 'yaw' 'yax' 'yay' 'yaz' 'yazz' 'yey' 'yi' 'yu']\n",
      "Epoch 1/15 - Train Loss: 0.0710 - Train Acc: 0.3902 - Val Loss: 0.0554 - Val Acc: 0.5064\n",
      "Epoch 2/15 - Train Loss: 0.0491 - Train Acc: 0.5528 - Val Loss: 0.0473 - Val Acc: 0.5721\n",
      "Epoch 3/15 - Train Loss: 0.0428 - Train Acc: 0.6045 - Val Loss: 0.0433 - Val Acc: 0.5991\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LeNet5:\n",
    "    def __init__(self, input_shape=(32, 32, 1), num_classes=33):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialisation des paramètres\n",
    "        self.params = {}\n",
    "        \n",
    "        # Couche C1: 6 filtres 5x5x1\n",
    "        self.params['C1_filters'] = np.random.randn(6, 5, 5, 1) * 0.1\n",
    "        \n",
    "        # Couche C3: 16 filtres 5x5x6\n",
    "        self.params['C3_filters'] = np.random.randn(16, 5, 5, 6) * 0.1\n",
    "        \n",
    "        # Couche C5: 120 filtres 5x5x16\n",
    "        self.params['C5_filters'] = np.random.randn(120, 5, 5, 16) * 0.1\n",
    "        \n",
    "        # Couche F6: 84 neurones\n",
    "        self.params['F6_weights'] = np.random.randn(120, 84) * 0.1\n",
    "        self.params['F6_bias'] = np.zeros(84)\n",
    "        \n",
    "        # Couche de sortie\n",
    "        self.params['output_weights'] = np.random.randn(84, num_classes) * 0.1\n",
    "        self.params['output_bias'] = np.zeros(num_classes)\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.m = {}  # Pour Adam optimizer\n",
    "        self.v = {}  # Pour Adam optimizer\n",
    "        self.t = 0   # Pour Adam optimizer\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_deriv(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def average_pooling(self, x, pool_size=2, stride=2):\n",
    "        n, h, w, c = x.shape\n",
    "        h_out = (h - pool_size) // stride + 1\n",
    "        w_out = (w - pool_size) // stride + 1\n",
    "        \n",
    "        output = np.zeros((n, h_out, w_out, c))\n",
    "        \n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                h_start = i * stride\n",
    "                h_end = h_start + pool_size\n",
    "                w_start = j * stride\n",
    "                w_end = w_start + pool_size\n",
    "                \n",
    "                pool_region = x[:, h_start:h_end, w_start:w_end, :]\n",
    "                output[:, i, j, :] = np.mean(pool_region, axis=(1, 2))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def conv2d(self, x, filters, stride=1):\n",
    "        n, h, w, c = x.shape\n",
    "        f_num, fh, fw, fc = filters.shape\n",
    "        assert c == fc, \"Nombre de canaux incompatible\"\n",
    "        \n",
    "        h_out = (h - fh) // stride + 1\n",
    "        w_out = (w - fw) // stride + 1\n",
    "        \n",
    "        output = np.zeros((n, f_num, h_out, w_out))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for f in range(f_num):\n",
    "                for hi in range(h_out):\n",
    "                    for wi in range(w_out):\n",
    "                        h_start = hi * stride\n",
    "                        h_end = h_start + fh\n",
    "                        w_start = wi * stride\n",
    "                        w_end = w_start + fw\n",
    "                        \n",
    "                        region = x[i, h_start:h_end, w_start:w_end, :]\n",
    "                        output[i, f, hi, wi] = np.sum(region * filters[f])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache['input'] = x\n",
    "        \n",
    "        # Couche C1\n",
    "        x = self.conv2d(x, self.params['C1_filters'])\n",
    "        x = self.tanh(x)\n",
    "        self.cache['C1_output'] = x\n",
    "        \n",
    "        # Reshape pour pooling\n",
    "        x = x.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Couche S2\n",
    "        x = self.average_pooling(x)\n",
    "        self.cache['S2_output'] = x\n",
    "        \n",
    "        # Couche C3\n",
    "        x = self.conv2d(x, self.params['C3_filters'])\n",
    "        x = self.tanh(x)\n",
    "        self.cache['C3_output'] = x\n",
    "        \n",
    "        # Reshape pour pooling\n",
    "        x = x.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Couche S4\n",
    "        x = self.average_pooling(x)\n",
    "        self.cache['S4_output'] = x\n",
    "        \n",
    "        # Couche C5\n",
    "        x = self.conv2d(x, self.params['C5_filters'])\n",
    "        x = self.tanh(x)\n",
    "        self.cache['C5_output'] = x\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        # Couche F6\n",
    "        x = np.dot(x, self.params['F6_weights']) + self.params['F6_bias']\n",
    "        x = self.tanh(x)\n",
    "        self.cache['F6_output'] = x\n",
    "        \n",
    "        # Couche de sortie\n",
    "        x = np.dot(x, self.params['output_weights']) + self.params['output_bias']\n",
    "        x = self.softmax(x)\n",
    "        self.cache['output'] = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backward(self, x, y, output):\n",
    "        # Gradient de la loss\n",
    "        dout = output - y\n",
    "        \n",
    "        # Couche de sortie\n",
    "        self.grads = {}\n",
    "        self.grads['output_weights'] = np.dot(self.cache['F6_output'].T, dout)\n",
    "        self.grads['output_bias'] = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Couche F6\n",
    "        dF6 = np.dot(dout, self.params['output_weights'].T) * self.tanh_deriv(self.cache['F6_output'])\n",
    "        self.grads['F6_weights'] = np.dot(self.cache['C5_output'].reshape(x.shape[0], -1).T, dF6)\n",
    "        self.grads['F6_bias'] = np.sum(dF6, axis=0)\n",
    "        \n",
    "        # Couche C5\n",
    "        dC5 = np.dot(dF6, self.params['F6_weights'].T).reshape(self.cache['C5_output'].shape)\n",
    "        dC5 = dC5 * self.tanh_deriv(self.cache['C5_output'])\n",
    "        \n",
    "        # ... (le reste de la backprop à implémenter de manière similaire)\n",
    "        \n",
    "    def update_params(self, optimizer, learning_rate):\n",
    "        if optimizer == 'sgd':\n",
    "            for param in self.params:\n",
    "                self.params[param] -= learning_rate * self.grads.get(param, 0)\n",
    "        elif optimizer == 'adam':\n",
    "            self.t += 1\n",
    "            beta1, beta2 = 0.9, 0.999\n",
    "            eps = 1e-8\n",
    "            \n",
    "            for param in self.params:\n",
    "                if param not in self.m:\n",
    "                    self.m[param] = np.zeros_like(self.params[param])\n",
    "                    self.v[param] = np.zeros_like(self.params[param])\n",
    "                \n",
    "                self.m[param] = beta1 * self.m[param] + (1 - beta1) * self.grads.get(param, 0)\n",
    "                self.v[param] = beta2 * self.v[param] + (1 - beta2) * (self.grads.get(param, 0)**2)\n",
    "                \n",
    "                m_hat = self.m[param] / (1 - beta1**self.t)\n",
    "                v_hat = self.v[param] / (1 - beta2**self.t)\n",
    "                \n",
    "                self.params[param] -= learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        epsilon = 1e-12\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred))\n",
    "    \n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, learning_rate=0.01, optimizer='sgd'):\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            \n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward\n",
    "                output = self.forward(X_batch)\n",
    "                \n",
    "                # Loss et accuracy\n",
    "                batch_loss = self.compute_loss(y_batch, output)\n",
    "                batch_acc = self.compute_accuracy(y_batch, output)\n",
    "                \n",
    "                epoch_loss += batch_loss * X_batch.shape[0]\n",
    "                epoch_acc += batch_acc * X_batch.shape[0]\n",
    "                \n",
    "                # Backward\n",
    "                self.backward(X_batch, y_batch, output)\n",
    "                \n",
    "                # Update\n",
    "                self.update_params(optimizer, learning_rate)\n",
    "            \n",
    "            # Métriques d'epoch\n",
    "            epoch_loss /= X_train.shape[0]\n",
    "            epoch_acc /= X_train.shape[0]\n",
    "            \n",
    "            # Validation\n",
    "            val_output = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, val_output)\n",
    "            val_acc = self.compute_accuracy(y_val, val_output)\n",
    "            \n",
    "            train_loss_history.append(epoch_loss)\n",
    "            train_acc_history.append(epoch_acc)\n",
    "            val_loss_history.append(val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {epoch_loss:.4f} - Train Acc: {epoch_acc:.4f} - \"\n",
    "                  f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
    "\n",
    "# Fonctions utilitaires restantes (load_tifinagh_dataset, prepare_data, plot_confusion_matrix, etc.)\n",
    "def load_tifinagh_dataset(data_path='amhcd-data-64/tifinagh-images/'):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(data_path))\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(class_names)\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "            \n",
    "        for img_file in os.listdir(class_path):\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                img = img.resize((32, 32))\n",
    "                img_array = np.array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    X = np.array(images).reshape(-1, 32, 32, 1)\n",
    "    y = label_encoder.transform(labels)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    y = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "    \n",
    "    return X, y, label_encoder.classes_\n",
    "\n",
    "def prepare_data():\n",
    "    X, y, class_names = load_tifinagh_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), class_names\n",
    "\n",
    "def plot_history(train_loss, train_acc, val_loss, val_acc):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(val_acc, label='Val Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exécution principale\n",
    "if __name__ == \"__main__\":\n",
    "    # Chargement des données\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), class_names = prepare_data()\n",
    "    \n",
    "    print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    \n",
    "    # Création du modèle\n",
    "    model = LeNet5(input_shape=(32, 32, 1), num_classes=len(class_names))\n",
    "    \n",
    "    # Entraînement\n",
    "    train_loss, train_acc, val_loss, val_acc = model.train(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    \n",
    "    # Visualisation\n",
    "    plot_history(train_loss, train_acc, val_loss, val_acc)\n",
    "    \n",
    "    # Évaluation\n",
    "    test_output = model.forward(X_test)\n",
    "    test_acc = model.compute_accuracy(y_test, test_output)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    y_pred = np.argmax(test_output, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045968b-e931-4992-bc69-e9ef45ad331c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
